{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b44b977",
   "metadata": {},
   "source": [
    "## Caltech - Machine Learning\n",
    "## Australian Apparel Ltd\n",
    "## Data Wrangling\n",
    "## Overview\n",
    "In this Jupyter Notebook, I perform the 'Data Wrangling' used to Load and Inspect the dataset.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### A.  Data Loading and Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59734abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Header 10 rows:\n",
      "         Date        Time State     Group  Unit  Sales\n",
      "0  1-Oct-2020     Morning    WA      Kids     8  20000\n",
      "1  1-Oct-2020     Morning    WA       Men     8  20000\n",
      "2  1-Oct-2020     Morning    WA     Women     4  10000\n",
      "3  1-Oct-2020     Morning    WA   Seniors    15  37500\n",
      "4  1-Oct-2020   Afternoon    WA      Kids     3   7500\n",
      "5  1-Oct-2020   Afternoon    WA       Men    10  25000\n",
      "6  1-Oct-2020   Afternoon    WA     Women     3   7500\n",
      "7  1-Oct-2020   Afternoon    WA   Seniors    11  27500\n",
      "8  1-Oct-2020     Evening    WA      Kids    15  37500\n",
      "9  1-Oct-2020     Evening    WA       Men    15  37500\n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7560 entries, 0 to 7559\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Date    7560 non-null   object\n",
      " 1   Time    7560 non-null   object\n",
      " 2   State   7560 non-null   object\n",
      " 3   Group   7560 non-null   object\n",
      " 4   Unit    7560 non-null   int64 \n",
      " 5   Sales   7560 non-null   int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 354.5+ KB\n",
      "None\n",
      "\n",
      "DataFrame Statistics:\n",
      "              Unit          Sales\n",
      "count  7560.000000    7560.000000\n",
      "mean     18.005423   45013.558201\n",
      "std      12.901403   32253.506944\n",
      "min       2.000000    5000.000000\n",
      "25%       8.000000   20000.000000\n",
      "50%      14.000000   35000.000000\n",
      "75%      26.000000   65000.000000\n",
      "max      65.000000  162500.000000\n",
      "\n",
      "Missing Values:\n",
      "Date     0\n",
      "Time     0\n",
      "State    0\n",
      "Group    0\n",
      "Unit     0\n",
      "Sales    0\n",
      "dtype: int64\n",
      "\n",
      "Incorrect Values:\n",
      "      Date  Time  State  Group  Unit  Sales\n",
      "0     True  True   True   True  True   True\n",
      "1     True  True   True   True  True   True\n",
      "2     True  True   True   True  True   True\n",
      "3     True  True   True   True  True   True\n",
      "4     True  True   True   True  True   True\n",
      "...    ...   ...    ...    ...   ...    ...\n",
      "7555  True  True   True   True  True   True\n",
      "7556  True  True   True   True  True   True\n",
      "7557  True  True   True   True  True   True\n",
      "7558  True  True   True   True  True   True\n",
      "7559  True  True   True   True  True   True\n",
      "\n",
      "[7560 rows x 6 columns]\n",
      "\n",
      "# of Duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Inspect the data\n",
    "def inspect_data(df):\n",
    "    print(\"Header 10 rows:\")\n",
    "    print(df.head(10))\n",
    "    \n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(\"\\nDataFrame Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    print(\"\\nIncorrect Values:\")\n",
    "    print(df.notna())\n",
    "\n",
    "    print(f\"\\n# of Duplicate rows: {df.duplicated().sum()}\")\n",
    "    \n",
    "    \n",
    "   \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# Load the data\n",
    "df = load_data('../data/AusApparalSales4thQrt2020.csv')\n",
    "\n",
    "# Inspect the data\n",
    "inspect_data(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8bf9b3",
   "metadata": {},
   "source": [
    "## Data Inspection Findings:\n",
    "- No missing values\n",
    "- 7560 rows across 6 columns [Date, Time, State, Group, Unit, Sales]\n",
    "- Date range is 10/01/2020 - 12/30/2020\n",
    "- Sales figures range from $5000 to $162500\n",
    "- Sales across 7 States\n",
    "- Sales across 4 Groups:  Kids, Men, Women, and Seniors\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### B.  Data Analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e86792a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values in Columns:\n",
      "\n",
      "Date:\n",
      "['1-Oct-2020' '2-Oct-2020' '3-Oct-2020' '4-Oct-2020' '5-Oct-2020'\n",
      " '6-Oct-2020' '7-Oct-2020' '8-Oct-2020' '9-Oct-2020' '10-Oct-2020'\n",
      " '11-Oct-2020' '12-Oct-2020' '13-Oct-2020' '14-Oct-2020' '15-Oct-2020'\n",
      " '16-Oct-2020' '17-Oct-2020' '18-Oct-2020' '19-Oct-2020' '20-Oct-2020'\n",
      " '21-Oct-2020' '22-Oct-2020' '23-Oct-2020' '24-Oct-2020' '25-Oct-2020'\n",
      " '26-Oct-2020' '27-Oct-2020' '28-Oct-2020' '29-Oct-2020' '30-Oct-2020'\n",
      " '1-Nov-2020' '2-Nov-2020' '3-Nov-2020' '4-Nov-2020' '5-Nov-2020'\n",
      " '6-Nov-2020' '7-Nov-2020' '8-Nov-2020' '9-Nov-2020' '10-Nov-2020'\n",
      " '11-Nov-2020' '12-Nov-2020' '13-Nov-2020' '14-Nov-2020' '15-Nov-2020'\n",
      " '16-Nov-2020' '17-Nov-2020' '18-Nov-2020' '19-Nov-2020' '20-Nov-2020'\n",
      " '21-Nov-2020' '22-Nov-2020' '23-Nov-2020' '24-Nov-2020' '25-Nov-2020'\n",
      " '26-Nov-2020' '27-Nov-2020' '28-Nov-2020' '29-Nov-2020' '30-Nov-2020'\n",
      " '1-Dec-2020' '2-Dec-2020' '3-Dec-2020' '4-Dec-2020' '5-Dec-2020'\n",
      " '6-Dec-2020' '7-Dec-2020' '8-Dec-2020' '9-Dec-2020' '10-Dec-2020'\n",
      " '11-Dec-2020' '12-Dec-2020' '13-Dec-2020' '14-Dec-2020' '15-Dec-2020'\n",
      " '16-Dec-2020' '17-Dec-2020' '18-Dec-2020' '19-Dec-2020' '20-Dec-2020'\n",
      " '21-Dec-2020' '22-Dec-2020' '23-Dec-2020' '24-Dec-2020' '25-Dec-2020'\n",
      " '26-Dec-2020' '27-Dec-2020' '28-Dec-2020' '29-Dec-2020' '30-Dec-2020']\n",
      "\n",
      "Time:\n",
      "['Morning' 'Afternoon' 'Evening']\n",
      "\n",
      "State:\n",
      "['WA' 'NT' 'SA' 'VIC' 'QLD' 'NSW' 'TAS']\n",
      "\n",
      "Group:\n",
      "['Kids' 'Men' 'Women' 'Seniors']\n",
      "Column 'Date' has no leading/trailing whitespaces.\n",
      "Column 'Time' has no leading/trailing whitespaces.\n",
      "Column 'State' has no leading/trailing whitespaces.\n",
      "Column 'Group' has no leading/trailing whitespaces.\n",
      "\n",
      "Unique values in State:\n",
      "['WA' 'NT' 'SA' 'VIC' 'QLD' 'NSW' 'TAS']\n",
      "\n",
      "Unique values in Group:\n",
      "['Kids' 'Men' 'Women' 'Seniors']\n",
      "\n",
      "Unique values in Time:\n",
      "['Morning' 'Afternoon' 'Evening']\n"
     ]
    }
   ],
   "source": [
    " print(\"\\nUnique values in Columns:\")\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df[col].unique())\n",
    "   \n",
    "# Remove leading or trailing whitespaces from categorical columns\n",
    "categorical_columns = ['State', 'Group', 'Time']\n",
    "for col in categorical_columns:\n",
    "    df[col] = df[col].str.strip()\n",
    "\n",
    "# Verify there a no leading or trailing whitespaces in String columns\n",
    "string_columns = df.select_dtypes(include=['object']).columns\n",
    "for col in string_columns:\n",
    "    if df[col].str.strip().ne(df[col]).any():\n",
    "        print(f\"Column '{col}' has leading/trailing whitespaces.\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' has no leading/trailing whitespaces.\")\n",
    "\n",
    "# Check for formatting inconsistencies in categorical variables\n",
    "categorical_columns = ['State', 'Group', 'Time']\n",
    "for col in categorical_columns:\n",
    "    print(f\"\\nUnique values in {col}:\")\n",
    "    print(df[col].unique())     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af8e4d",
   "metadata": {},
   "source": [
    "## Data Analytics findings\n",
    "- I found there were Leading spaces in Group and State columns, added code to remove spaces.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### C.  Data Wrangling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df91904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics for Unit:\n",
      "count    7560.000000\n",
      "mean       18.005423\n",
      "std        12.901403\n",
      "min         2.000000\n",
      "25%         8.000000\n",
      "50%        14.000000\n",
      "75%        26.000000\n",
      "max        65.000000\n",
      "Name: Unit, dtype: float64\n",
      "\n",
      "Statistics for Sales:\n",
      "count      7560.000000\n",
      "mean      45013.558201\n",
      "std       32253.506944\n",
      "min        5000.000000\n",
      "25%       20000.000000\n",
      "50%       35000.000000\n",
      "75%       65000.000000\n",
      "max      162500.000000\n",
      "Name: Sales, dtype: float64\n",
      "\n",
      "Statistics for Sales_Normalized:\n",
      "count    7560.000000\n",
      "mean        0.254054\n",
      "std         0.204784\n",
      "min         0.000000\n",
      "25%         0.095238\n",
      "50%         0.190476\n",
      "75%         0.380952\n",
      "max         1.000000\n",
      "Name: Sales_Normalized, dtype: float64\n",
      "\n",
      "Statistics for Unit_Normalized:\n",
      "count    7560.000000\n",
      "mean        0.254054\n",
      "std         0.204784\n",
      "min         0.000000\n",
      "25%         0.095238\n",
      "50%         0.190476\n",
      "75%         0.380952\n",
      "max         1.000000\n",
      "Name: Unit_Normalized, dtype: float64\n",
      "               Sales         Unit\n",
      "count    7560.000000  7560.000000\n",
      "mean    45013.558201    18.005423\n",
      "std     32253.506944    12.901403\n",
      "min      5000.000000     2.000000\n",
      "25%     20000.000000     8.000000\n",
      "50%     35000.000000    14.000000\n",
      "75%     65000.000000    26.000000\n",
      "max    162500.000000    65.000000\n",
      "\n",
      "First few rows with normalized columns:\n",
      "   Sales  Sales_Normalized  Unit  Unit_Normalized\n",
      "0  20000          0.095238     8         0.095238\n",
      "1  20000          0.095238     8         0.095238\n",
      "2  10000          0.031746     4         0.031746\n",
      "3  37500          0.206349    15         0.206349\n",
      "4   7500          0.015873     3         0.015873\n",
      "\n",
      "Summary of normalized columns:\n",
      "       Sales_Normalized  Unit_Normalized\n",
      "count       7560.000000      7560.000000\n",
      "mean           0.254054         0.254054\n",
      "std            0.204784         0.204784\n",
      "min            0.000000         0.000000\n",
      "25%            0.095238         0.095238\n",
      "50%            0.190476         0.190476\n",
      "75%            0.380952         0.380952\n",
      "max            1.000000         1.000000\n"
     ]
    }
   ],
   "source": [
    "# Identify the numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Validate data ranges for numerical columns\n",
    "for col in numerical_columns:\n",
    "    print(f\"\\nStatistics for {col}:\")\n",
    "    print(df[col].describe())\n",
    "    \n",
    "# Check for negative values in columns\n",
    "if col in ['Unit', 'Sales']:\n",
    "    neg_count = (df[col] < 0).sum()\n",
    "    if neg_count > 0:\n",
    "        print(f\"Warning: {neg_count} negative values found in {col}\")\n",
    "    else:\n",
    "        print(f\"No negative values found in {col}\")\n",
    "# Check for outliers or incorrect data\n",
    "print(df[['Sales', 'Unit']].describe())\n",
    "\n",
    "# Simple normalization for 'Sales' and 'Unit' columns\n",
    "df['Sales_Normalized'] = (df['Sales'] - df['Sales'].min()) / (df['Sales'].max() - df['Sales'].min())\n",
    "df['Unit_Normalized'] = (df['Unit'] - df['Unit'].min()) / (df['Unit'].max() - df['Unit'].min())\n",
    "\n",
    "print(\"\\nFirst few rows with normalized columns:\")\n",
    "print(df[['Sales', 'Sales_Normalized', 'Unit', 'Unit_Normalized']].head())\n",
    "\n",
    "print(\"\\nSummary of normalized columns:\")\n",
    "print(df[['Sales_Normalized', 'Unit_Normalized']].describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e3b15",
   "metadata": {},
   "source": [
    "## Data Wrangling findings\n",
    "- Utilized Min/Max for Normalization to scale values from 0 to 1.\n",
    "- A Mean value of 0.254 or 25.4% of the max values\n",
    "- A Median value of 0.190 which is less than Mean of 0.254 which indicates a positive (right) skewing of the distribution\n",
    "- A Standard Deviation of .204 indicates a slight variability in the distribution.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "### D.  Data Chunking or Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a5c6044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics by State:\n",
      "        count          mean           std      min      25%      50%  \\\n",
      "State                                                                  \n",
      "NSW    1080.0  69416.666667  20626.651646  30000.0  52500.0  70000.0   \n",
      "NT     1080.0  20907.407407   8961.907893   5000.0  15000.0  20000.0   \n",
      "QLD    1080.0  30942.129630  13344.638002   7500.0  20000.0  30000.0   \n",
      "SA     1080.0  54497.685185  17460.965183  25000.0  40000.0  52500.0   \n",
      "TAS    1080.0  21074.074074   9024.684205   5000.0  15000.0  20000.0   \n",
      "VIC    1080.0  97745.370370  26621.597092  50000.0  77500.0  95000.0   \n",
      "WA     1080.0  20511.574074   9231.905897   5000.0  12500.0  20000.0   \n",
      "\n",
      "            75%       max  \n",
      "State                      \n",
      "NSW     85000.0  112500.0  \n",
      "NT      27500.0   37500.0  \n",
      "QLD     40000.0   62500.0  \n",
      "SA      67500.0   87500.0  \n",
      "TAS     27500.0   37500.0  \n",
      "VIC    112500.0  162500.0  \n",
      "WA      27500.0   37500.0  \n",
      "\n",
      "Summary statistics by Customer Grouping:\n",
      "          count          mean           std     min      25%      50%  \\\n",
      "Group                                                                   \n",
      "Kids     1890.0  45011.904762  31871.491085  5000.0  20000.0  35000.0   \n",
      "Men      1890.0  45370.370370  32177.180712  5000.0  20000.0  35000.0   \n",
      "Seniors  1890.0  44464.285714  32195.360017  5000.0  20000.0  35000.0   \n",
      "Women    1890.0  45207.671958  32781.639869  5000.0  20000.0  35000.0   \n",
      "\n",
      "             75%       max  \n",
      "Group                       \n",
      "Kids     65000.0  162500.0  \n",
      "Men      65000.0  160000.0  \n",
      "Seniors  62500.0  162500.0  \n",
      "Women    67500.0  162500.0  \n",
      "Monthly Sales Summaries:\n",
      "           Date  Total_Sales  Avg_Daily_Sales  Num_Transactions  Total_Units  \\\n",
      "0    1-Dec-2020      4465000     53154.761905                84         1786   \n",
      "1    1-Nov-2020      3020000     35952.380952                84         1208   \n",
      "2    1-Oct-2020      3720000     44285.714286                84         1488   \n",
      "3   10-Dec-2020      4717500     56160.714286                84         1887   \n",
      "4   10-Nov-2020      2992500     35625.000000                84         1197   \n",
      "..          ...          ...              ...               ...          ...   \n",
      "85   8-Nov-2020      3210000     38214.285714                84         1284   \n",
      "86   8-Oct-2020      3945000     46964.285714                84         1578   \n",
      "87   9-Dec-2020      4655000     55416.666667                84         1862   \n",
      "88   9-Nov-2020      2925000     34821.428571                84         1170   \n",
      "89   9-Oct-2020      3645000     43392.857143                84         1458   \n",
      "\n",
      "    Avg_Daily_Units  Sales_Growth_Rate  \n",
      "0         21.261905                NaN  \n",
      "1         14.380952         -32.362822  \n",
      "2         17.714286          23.178808  \n",
      "3         22.464286          26.814516  \n",
      "4         14.250000         -36.565978  \n",
      "..              ...                ...  \n",
      "85        15.285714         -29.912664  \n",
      "86        18.785714          22.897196  \n",
      "87        22.166667          17.997465  \n",
      "88        13.928571         -37.164339  \n",
      "89        17.357143          24.615385  \n",
      "\n",
      "[90 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Group By State\n",
    "state_grouping = df.groupby('State')\n",
    "\n",
    "# Display summary  fo each state\n",
    "print(\"Summary statistics by State:\")\n",
    "print(state_grouping['Sales'].describe())\n",
    "\n",
    "# Group By Customer Group\n",
    "customer_grouping = df.groupby('Group')\n",
    "\n",
    "# Display summary for each customer grouping\n",
    "print(\"\\nSummary statistics by Customer Grouping:\")\n",
    "print(customer_grouping['Sales'].describe())\n",
    "\n",
    "# Group by Month and calculate monthly summaries\n",
    "monthly_sales = df.groupby('Date').agg({\n",
    "    'Sales': ['sum', 'mean', 'count'],\n",
    "    'Unit': ['sum', 'mean']\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "monthly_sales.columns = ['Total_Sales', 'Avg_Daily_Sales', 'Num_Transactions', 'Total_Units', 'Avg_Daily_Units']\n",
    "\n",
    "# Reset index to make 'Month' a column\n",
    "monthly_sales = monthly_sales.reset_index()\n",
    "\n",
    "# Calculate Monthly growth rate for Total_Sales\n",
    "monthly_sales['Sales_Growth_Rate'] = monthly_sales['Total_Sales'].pct_change() * 100\n",
    "\n",
    "# Display the monthly summaries\n",
    "print(\"Monthly Sales Summaries:\")\n",
    "print(monthly_sales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5261ed",
   "metadata": {},
   "source": [
    "## Data Chunking and Merging findings\n",
    "- Usage of the GroupBy function to Analyze the data\n",
    "- Sales seem to peak in December likely due to Holiday sales"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
